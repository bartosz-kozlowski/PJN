{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mooJ5vGuciH3"
   },
   "source": [
    "# Atencja — implementacja atencji między danym stanem dekodera a stanami kodera\n",
    "\n",
    "Poniżej znajdują się dwie macierze `encoder_states` i `decoder_states` reprezentujące stan warstwy ukrytej po przetworzeniu każdego słowa przez koder oraz statyczne wektory zanurzeń związane z danym wejściem dekodera. Pojedynczy stan warstwy ukrytej zawiera zanurzenia o długości = 3, która to liczba jest równa rozmiarowi zanurzenia w dekoderze. W koderze mamy 4 stany warstw ukrytych, ponieważ przetwarzamy sekwencję składającą się z 4 tokenów.\n",
    "\n",
    "W dekoderze znajduje się 5 tokenów, które są generowane na podstawie sekwencji przetwarzanej przez koder.\n",
    "\n",
    "*Zadanie (1 punkt)*\n",
    "\n",
    "Zadanie polega na: a) Obliczeniu podobieństwa wszystkich zanurzeń z dekodera (zapytań -- queries) do wszystkich zanurzeń kolejnych stanów kodera (kluczy -- keys) (pamiętaj, że macierze można transponować. W NumPy transponujemy macierz za pomocą `nazwa_macierzy. T`)\n",
    "\n",
    "a) Softmax (zaimportowany z scipy) należy wykonać na utworzonej macierzy podobieństw. Uwaga: pamiętaj, aby zastosować softmax we właściwym wymiarze. Wszystkie ukryte stany kodera powinny być zmiękczone w kontekście danego stanu dekodera. W scipy funkcja softmax zawiera argument `axis`, który może pomóc.\n",
    "\n",
    "b) Połącz macierz uwagi z kroku b) i „encoder_states”, aby wygenerować macierz zawierającą wektory kontekstu dla każdego tokena z dekodera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Vbq0QW2td41r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.806   2.376   7.762   1.129]\n",
      " [ 12.164 -12.645  73.935   3.636]\n",
      " [ 27.79  -16.962  94.002   7.137]\n",
      " [ 18.702  -5.184  42.616   4.501]\n",
      " [ 64.38   49.86   56.21   14.45 ]]\n",
      "[[4.91780633e-02 4.32948093e-03 9.45248312e-01 1.24414389e-03]\n",
      " [1.49003187e-27 2.50486173e-38 1.00000000e+00 2.94803216e-31]\n",
      " [1.75587568e-29 6.44090821e-49 1.00000000e+00 1.88369172e-38]\n",
      " [4.11416552e-11 1.74069934e-21 1.00000000e+00 2.79811669e-17]\n",
      " [9.99716568e-01 4.94220792e-07 2.82937800e-04 2.06801368e-22]]\n",
      "[[ 9.69108631  0.35799187  0.59163688]\n",
      " [10.2         0.2         0.3       ]\n",
      " [10.2         0.2         0.3       ]\n",
      " [10.2         0.2         0.3       ]\n",
      " [ 1.20254471  3.39909302  5.59850122]]\n"
     ]
    }
   ],
   "source": [
    "#!pip install scipy\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "# scipy.special.softmax(x, axis=None)\n",
    "\n",
    "encoder_states = np.array(\n",
    "    [[1.2, 3.4, 5.6],   # encoder's hidden layer output at the step 1, related to a given input token, e.g., I\n",
    "    [-2.3, 0.2, 7.2],   # encoder's hidden layer output at the step 2, related to a given token, e.g., like\n",
    "    [10.2, 0.2, 0.3],   # encoder's hidden layer output at the step 3, related to a given token, e.g., NLP\n",
    "    [0.4, 0.7, 1.2]]    # encoder's hidden layer output at the step 4, related to a given token, e.g., \".\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "decoder_states = np.array(\n",
    "    [[0.74, 0.23, 0.56],  # decoder's static word embedding at the step 1, related to a given token, e.g., <BOS>\n",
    "    [7.23, 0.12, 0.55],  # decoder's static word embedding at the step 2, related to a given token, e.g., Ja\n",
    "    [9.12, 4.23, 0.44], # decoder's static word embedding at the step 3, related to a given token, e.g., lubię\n",
    "    [4.1, 3.23, 0.5],    # decoder's static word embedding at the step 4, related to a given token, e.g., przetwarzanie\n",
    "    [5.2, 3.1, 8.5]]     # decoder's static word embedding at the step 5, related to a given token, e.g., języka\n",
    ")\n",
    "\n",
    "attention_scores = np.dot(decoder_states, encoder_states.T)\n",
    "attention_weights = softmax(attention_scores, axis=1)\n",
    "context_vectors = np.dot(attention_weights, encoder_states)\n",
    "print(attention_scores)\n",
    "print(attention_weights)\n",
    "print(context_vectors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8bfd5X4fAJq"
   },
   "source": [
    "Oczekiwane wyjścia:\n",
    "\n",
    "a) [[ 4.806 2.376 7.762 1.129] [ 12.164 -12.645 73.935 3.636] [ 27.79 -16.962 94.002 7.137] [ 18.702 -5.184 42.616 4.501] [ 64.38 49.86 56.21 14.45 ]]\n",
    "\n",
    "b) [[4.91780633e-02 4.32948093e-03 9.45248312e-01 1.24414389e-03] [1.49003187e-27 2.50486173e-38 1.00000000e+00 2.94803216e-31] [1.75587568e-29 6.44090821e-49 1.00000000e+00 1.88369172e-38] [4.11416552e-11 1.74069934e-21 1.00000000e+00 2.79811669e-17] [9.99716568e-01 4.94220792e-07 2.82937800e-04 2.06801368e-22]]\n",
    "\n",
    "c) [[ 9.69108631 0.35799187 0.59163688] [10.2 0.2 0.3 ] [10.2 0.2 0.3 ] [10.2 0.2 0.3 ] [ 1.20254471 3.39909302 5.59850122]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sPUDVw7fKHJ"
   },
   "source": [
    "# Transformer\n",
    "## Wykorzystanie modelu T5 opartego na transformerze do rozwiązywania różnych zadań NLP.\n",
    "\n",
    "Dzisiaj poznamy nową bibliotekę — HuggingFace **transformers** (https://huggingface.co/docs/transformers/index) i użyjemy jej do rozwiązania kilku nieoczywistych problemów związanych z NLP za pomocą modelu **T5** .\n",
    "\n",
    "\n",
    "HuggingFace transformers to jedna z najpopularniejszych bibliotek dostarczających nam wysokopoziomowe API do wykorzystania sieci neuronowych do rozwiązywania zadań związanych z przetwarzaniem języka naturalnego, przetwarzaniem dźwięku, wizją komputerową, a nawet scenariuszami multimodalnymi, w których musimy wykorzystywać wiele modalności naraz (np. odpowiadając na pytania o zdjęcia, wydobywając informacje z faktur).\n",
    "\n",
    "Najpierw zainstalujmy zależności, samą bibliotekę `transformers` oraz moduł `sentencepiece`, który pomaga nam tokenizować dokumenty i przekształcać tokeny w kodowanie one-hot (ideę sentencepiece omówimy szczegółowo później).\n",
    "\n",
    "**Ostrzeżenie**: jeśli zauważysz jakieś dziwne wyjątki, takie jak `cannot call from_pretrained na obiekcie None` gdzieś w twoim kodzie, zrestartuj środowisko używając: Runtime -> restart. Następnie uruchom komórki z kodem (bez ponownej instalacji bibliotek) jeszcze raz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jmRzPimhfRja"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\48663\\anaconda3\\envs\\t5env\\lib\\site-packages (4.50.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\48663\\anaconda3\\envs\\t5env\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\48663\\anaconda3\\envs\\t5env\\lib\\site-packages (from transformers) (0.30.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\48663\\anaconda3\\envs\\t5env\\lib\\site-packages (from transformers) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\48663\\anaconda3\\envs\\t5env\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\48663\\anaconda3\\envs\\t5env\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\48663\\anaconda3\\envs\\t5env\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\48663\\anaconda3\\envs\\t5env\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\48663\\anaconda3\\envs\\t5env\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\48663\\anaconda3\\envs\\t5env\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\48663\\anaconda3\\envs\\t5env\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\48663\\anaconda3\\envs\\t5env\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\48663\\anaconda3\\envs\\t5env\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.13.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\48663\\anaconda3\\envs\\t5env\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\48663\\anaconda3\\envs\\t5env\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\48663\\anaconda3\\envs\\t5env\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\48663\\anaconda3\\envs\\t5env\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\48663\\anaconda3\\envs\\t5env\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.2.0-cp310-cp310-win_amd64.whl.metadata (8.3 kB)\n",
      "Using cached sentencepiece-0.2.0-cp310-cp310-win_amd64.whl (991 kB)\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wSAg0LOEgGj4"
   },
   "source": [
    "Przeczytaj dokumentację dotyczącą modelu T5 dostępne tutaj: https://huggingface.co/docs/transformers/model_doc/t5\n",
    "\n",
    "W sekcji `wnioskowanie` znajdziesz opis pokazujący w jaki sposób możemy pobrać wytrenowany model i użyć go do rozwiązania zadanego zadania. Po prostu użyj dostarczonego kodu, aby przetłumaczyć jakieś zdanie z angielskiego na niemiecki!\n",
    "\n",
    "*(0.5 punkta)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "lYOfsrkeJ6GF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ich erinnere mich nicht an\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")\n",
    "\n",
    "input_ids = tokenizer(\"translate English to German: I do not remember\", return_tensors=\"pt\").input_ids\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z322IklnhQnO"
   },
   "source": [
    "## Różne zadania\n",
    "\n",
    "Eksperymentuj z innymi danymi wejściowymi, np. tymi, które przedstawiono na rysunku 1 przedstawionym w artykule przedstawiającym model T5 lub nawet szerszą listą przypadków użycia z Dodatku D dostarczonego z artykułem. Artykuł można znaleźć tutaj: https://arxiv.org/pdf/1910.10683.pdf\n",
    "\n",
    "Uwaga: wśród dostarczonych danych wejściowych zastosowano kilka skrótów, niektóre z nich to:\n",
    "- `stsb`: oznacza semantyczny test porównawczy podobieństwa tekstu. Biorąc pod uwagę dwa zdania, możemy obliczyć ich podobieństwa semantyczne, co może pomóc nam ustalić, czy jedno zdanie jest parafrazą drugiego.\n",
    "- `cola`: oznacza Corpus of Linguistic Acceptability i pomaga nam określić, czy dane zdanie jest gramatyczne, czy niegramatyczne.\n",
    "\n",
    "Jeśli spojrzysz na Dodatek D, skrótów jest więcej, są one związane z nazwami zadań przedstawionymi w benchmarku GLUE (dostępny tutaj: https://gluebenchmark.com/tasks) i benchmarku SUPERGLUE (dostępny tutaj: https:/ /super.gluebenchmark.com/tasks). Ideą GLUE i SUPERGLUE jest zebranie zestawu trudnych zadań, które można wykorzystać do oceny systemów wymagających rozumienia języka naturalnego.\n",
    "\n",
    "**Wklej 3 przykładowe zadania i przetworzone dane wejściowe w komórce poniżej (1 punkt)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OfxmQwV4lIXw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STSB result: 3.6\n",
      "Grammar check result: acceptable\n",
      "Paraphrase result: True\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "input_text = \"stsb sentence1: A man is playing a guitar. sentence2: A person is playing an instrument.\"\n",
    "input_text2 = \"cola sentence: The cats eats food.\"\n",
    "input_text3 = \"paraphrase sentence1: Do you believe in life after death? sentence2: Do you think there's something after we die?\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "inputs_1 = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs_2 = tokenizer(input_text2, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs_3 = tokenizer(input_text3, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs_1 = model.generate(inputs_1.input_ids)\n",
    "outputs_2 = model.generate(inputs_2.input_ids)\n",
    "outputs_3 = model.generate(inputs_3.input_ids)\n",
    "\n",
    "decoded_output_1 = tokenizer.decode(outputs_1[0], skip_special_tokens=True)\n",
    "decoded_output_2 = tokenizer.decode(outputs_2[0], skip_special_tokens=True)\n",
    "decoded_output_3 = tokenizer.decode(outputs_3[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"STSB result:\", decoded_output_1)\n",
    "print(\"Grammar check result:\", decoded_output_2)\n",
    "print(\"Paraphrase result:\", decoded_output_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQvUkC-BlW3q"
   },
   "source": [
    "## Różne typy modeli\n",
    "\n",
    "Dostępnych jest kilka modeli T5, które różnią się wielkością (i jakością). Im większy model, tym lepsze wyjście powinien generować. Eksperymentuj z niektórymi modelami z następującego zestawu:\n",
    "- t5-small\n",
    "- base t5\n",
    "- t5-large\n",
    "- t5-3b\n",
    "- t5-11b\n",
    "\n",
    "Sprawdź, czy można zaobserwować różnicę w jakości generowanych tekstów.\n",
    "\n",
    "Porównaj również rozmiar modeli, możesz użyć funkcji `model.num_parameters()`, aby uzyskać numer parametru związany z każdym modelem. Dla każdego modelu, który jesteś w stanie załadować, podaj rozmiar w poniższej komórce (jeśli nie możesz załadować danego modelu, bo jest za duży, bez obaw, po prostu wpisz 'too big to load'). (*2 punkty*) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1MTMhyyeR3ZJ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\48663\\anaconda3\\envs\\t5env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t5-small params number: 60506624\n",
      "t5-base params number: 222903552\n",
      "t5-large params number: 737668096\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "models = [\"t5-small\", \"t5-base\", \"t5-large\", \"t5-3b\", \"t5-11b\"]\n",
    "for model_name in models:\n",
    "    try:\n",
    "        tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "        num_params = model.num_parameters()\n",
    "\n",
    "        print(f\"{model_name} params number: {num_params}\")\n",
    "    except:\n",
    "        print(f\"{model_name} - too big to load\")\n",
    "# t5-small params number: 60506624\n",
    "# t5-base params number: 222903552\n",
    "# t5-large params number: 737668096\n",
    "# t5-3b params number: too big to load\n",
    "# t5-11b params number: too big to load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDud66l6msUI"
   },
   "source": [
    "## T5 specyficzne dla języka (ZADANIE OPCJONALNE – nie musisz tutaj podawać kodu)\n",
    "\n",
    "Istnieją nawet alternatywy dla oryginalnych modeli T5. Ponieważ model T5 był trenowany na języku angielskim, dostępne są modele specyficzne dla innych języków, np. polskiego (np. plT5 zaproponowany przez Allegro - https://huggingface.co/allegro/plt5-small). Polski model został przeszkolony do rozwiązywania zestawu zadań zebranych w benchmarku KLEJ, który stanowi polską analogię do benchmarku GLUE: https://klejbenchmark.com.\n",
    "\n",
    "Więcej szczegółów na temat plT5 można znaleźć w artykule badawczym: https://arxiv.org/pdf/2205.08808.pdf. Tabela 2 przedstawia przykładowe podpowiedzi, które można wykorzystać do rozwiązania niektórych zadań wymienionych w KLEJ.\n",
    "\n",
    "Możesz wyszukać alternatywę dla oryginalnego T5, na przykład tę związaną z Twoim językiem, i poeksperymentować z nią (**to zadanie nie jest obowiązkowe**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJZZUkduoEhZ"
   },
   "outputs": [],
   "source": [
    "# (OPTIONAL): If you want, experiment with some alternative models (like language-related, e.g., plT5 related to Polish)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1hsTnxboIe8"
   },
   "source": [
    "## Flan-T5\n",
    "\n",
    "Pod koniec 2022 roku zaproponowano ewolucję T5 o nazwie Flan-T5. Ten model jest również dostarczany przez bibliotekę transformatorów HuggingFace. Odwiedź tę stronę: https://huggingface.co/docs/transformers/model_doc/flan-t5, aby zobaczyć, jak możesz użyć tego modelu (wystarczy zmienić nazwę modelu, aby pobrać!).\n",
    "\n",
    "Flan-T5 jest znacznie potężniejszy niż T5. Możesz zajrzeć do Dodatku D zawartego w artykule opisującym Flan T5, aby zapoznać się z niektórymi formatami wejściowymi (monitami) i generowanymi wartościami. Artykuł jest tutaj: https://arxiv.org/pdf/1910.10683.pdf. Powinieneś skupić się na polach „przetworzonych danych wejściowych”, ponieważ są to reprezentacje używane przez model. Eksperymentuj z wybranymi zadaniami i sprawdź, czy możesz uzyskać takie same wyniki! W poniższym kodzie wklej kod ładujący model Flan-T5 i wykorzystujący go do rozwiązania wybranych zadań. (*1 punkt*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ppbmGGqVqERf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\48663\\anaconda3\\envs\\t5env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\48663\\anaconda3\\envs\\t5env\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\48663\\.cache\\huggingface\\hub\\models--google--flan-t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Zadanie:\n",
      "Translate English to French: The universe is vast and mysterious.\n",
      "Odpowiedź modelu:\n",
      "La universe est d'une grande et d'une sénoncé.\n",
      "\n",
      "Zadanie:\n",
      "Summarize: The Eiffel Tower is one of the most famous landmarks in Paris. It was built in 1889 and attracts millions of tourists each year.\n",
      "Odpowiedź modelu:\n",
      "The Eiffel Tower is one of the most famous landmarks in Paris.\n",
      "\n",
      "Zadanie:\n",
      "Answer the question: Who wrote the novel '1984'?\n",
      "Odpowiedź modelu:\n",
      "john scott\n",
      "\n",
      "Zadanie:\n",
      "Classify the sentiment: I absolutely loved the movie and would watch it again!\n",
      "Odpowiedź modelu:\n",
      "positive\n",
      "\n",
      "Zadanie:\n",
      "Is the following sentence grammatical? She go to school every day.\n",
      "Odpowiedź modelu:\n",
      "no\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "model_name = \"google/flan-t5-small\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tasks = [\n",
    "    \"Translate English to French: The universe is vast and mysterious.\",\n",
    "    \"Summarize: The Eiffel Tower is one of the most famous landmarks in Paris. It was built in 1889 and attracts millions of tourists each year.\",\n",
    "    \"Answer the question: Who wrote the novel '1984'?\",\n",
    "    \"Classify the sentiment: I absolutely loved the movie and would watch it again!\",\n",
    "    \"Is the following sentence grammatical? She go to school every day.\"\n",
    "]\n",
    "for task in tasks:\n",
    "    input_ids = tokenizer(task, return_tensors=\"pt\").input_ids\n",
    "    outputs = model.generate(input_ids, max_length=100)\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\nZadanie:\\n{task}\")\n",
    "    print(f\"Odpowiedź modelu:\\n{decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwyRmBx6q0iT"
   },
   "source": [
    "## (OPCJONALNIE) Dostrajanie\n",
    "\n",
    "Możesz nawet dostroić model T5/Flan-T5, aby rozwiązać wybrane zadanie. Możesz załadować istniejący model T5/Flan-T5, który jest już przeszkolony do rozwiązywania niektórych zadań, i wykorzystać moc 'transfery wiedzy', aby nauczyć go rozwiązywać różne zadania. Jest to o wiele lepsze niż trenowanie sieci od zera i powinno wymagać mniejszej liczby przykładów szkoleniowych.\n",
    "\n",
    "Faza dostrajania jest dość złożona. Jednak opis krok po kroku można znaleźć tutaj: https://www.philschmid.de/fine-tune-flan-t5\n",
    "\n",
    "Możesz spróbować dostroić wybrany model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "frt5p4E_rjGd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
