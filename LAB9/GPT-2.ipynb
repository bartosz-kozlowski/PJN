{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0w5N-ULYslb"
   },
   "source": [
    "# Generowanie tekstu za pomocą GPT-2\n",
    "\n",
    "Dzisiaj spróbujemy wygenerować teksty z wykorzystaniem modelu GPT-2 zaproponowanego przez OpenAI. Model ten jest pochodną transformera. Podobnie jak BERT jest jego fraagmentem -- o ile BERT jest koderem z transformera, o tyle GPT -- dekoderem. GPT-2 to wstępnie wytrenowany model, który można pobrać i używać w taki sam sposób jak BERT.\n",
    "\n",
    "Tutaj możesz znaleźć świetne wprowadzenie do ogólnej idei GPT-2: https://jalammar.github.io/illustrated-gpt2/\n",
    "\n",
    "Generalnie jest to model językowy, model, który daje nam prawdopodobieństwo tego, że dany token jest kontynuacją zadanego kontekstu. Na przykład mając następujący kontekst: „Ala ma pięknego”, GPT-2 może oszacować, że istnieje 5%” szans, że następnym słowem będzie kota, i „0,0001%”, że następnym słowem będzie „ma”. `.\n",
    "\n",
    "Wykorzystamy bibliotekę `Huggingface Transformers` do eksperymentowania z GPT-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7LwgDoMiINYf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\48663\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.50.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\48663\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\48663\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.30.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\48663\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\48663\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\48663\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\48663\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\48663\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\48663\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\48663\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\48663\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\48663\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\48663\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\48663\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\48663\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\48663\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\48663\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\48663\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2023.7.22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrgKFTxbasR0"
   },
   "source": [
    "# PODSTAWOWE GENEROWANIE TEKSTU (2 punkty)\n",
    "\n",
    "Zacznijmy od podstawowego scenariusza — ponieważ GPT-2 może obliczyć prawdopodobieństwo wystąpienia następnego słowa po zadanym kontekście, może być używany do generowania tekstów. W bibliotece `transformers` możemy to zrobić dość łatwo. `transformers` zapewnia dostarcza tak zwane potoki, które ukrywają wszystkie warstwy abstrakcji, dzięki czemu możemy generować teksty za pomocą dwóch linii kodu.\n",
    "\n",
    "Przeczytaj dokumentację, która znajduje się tutaj: https://huggingface.co/docs/transformers/v4.19.2/en/main_classes/pipelines, aby zapoznać się z potokami.\n",
    "\n",
    "Następnie wypełnij poniższy kod odpowiednimi fragmentami. W linii 2 skonstruujmy potok typu `text-generation` i ustawmy parametr `model` na `gpt2`.\n",
    "\n",
    "Następnie `generator` można wywoływać w taki sam sposób jak funkcję dając po nim nawiasy okrągłe z parametrami `generator(__tutaj  parametry__)`. Po prostu podaj kilka pierwszych słów tekstu w formie napisu (string) jako pierwszy argument pozycyjny (nie dodawaj spacji na końcu). Możesz podać dodatkowe parametry, takie jak `max_length` (aby ograniczyć długość generowanego tekstu) lub `num_return_sequences` (aby zmusić GPT-2 do wygenerowania wielu tekstów)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "D7QxEKB_IOZ1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Wygenerowany tekst 1 ===\n",
      "Once upon a time the King was able to hear them and knew of their whereabouts and to give them a small token of protection. At this time, in order to put an end to the trouble in which they came into existence he brought some priests who\n",
      "\n",
      "=== Wygenerowany tekst 2 ===\n",
      "Once upon a time, people always seemed like they needed more of these kinds of things but you look back at that in a way that you can probably trace back to the Renaissance or you see the decline in it, I think it was about the beginning\n",
      "\n",
      "=== Wygenerowany tekst 3 ===\n",
      "Once upon a time, with all parties able to help. The two parties that were able to be there at the time were the police and the police officers, and the party that went to try to calm the situation would not be able to stop.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")   # construct text-generation pipeline with model set to gpt2\n",
    "output = generator(\"Once upon a time\", truncation = True, max_length=50, num_return_sequences=3)\n",
    "for i, text in enumerate(output):\n",
    "    print(f\"=== Wygenerowany tekst {i+1} ===\")\n",
    "    print(text['generated_text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tE7Utv2j3k8"
   },
   "source": [
    "Istnieją różne modele oparte na GPT, które są dostępne w bibliotece `transformers`. Tutaj: https://huggingface.co/models?search=gpt, znajdziesz ich listę. Różnią się zestawami danych, na których zostały wytrenowane (oryginalny GPT-2 został wytrenowany na Webtext https://paperswithcode.com/dataset/webtext, który składa się z ~ 40 GB tekstów ściągniętych z Internetu) i rozmiarami modeli (np. GPT2-small składa się z 117M parametrów, GPT2-medium z 345M, GPT2-large z 762M).\n",
    "\n",
    "W zależności od naszych potrzeb i dostępnej pamięci GPU, możemy wybrać odpowiednią wielkość.\n",
    "Istnieją również modele destylowane, które są `skompresowane` podobnie jak np. popularny model DistilBERT: https://huggingface.co/distilgpt2 (Więcej o destylacji znajdziesz tutaj: https://neptune.ai/blog/knowledge-distillation).\n",
    "\n",
    "Sprawdź jak modele o różnej wielkości mają się do jakości generowanych tekstów. Użyj `gpt2-small`, `gpt2-medium`, `gpt2-large` zamiast `gpt2` w potoku i przeanalizuj wyniki.\n",
    "\n",
    "Sprawdź, jak działają modele wytrenowane na bardziej „konkretnych” danych (np.\n",
    "`CodeGPT-small-java-adaptedGPT2`, którego można użyć do pisania kodu w Javie)\n",
    "\n",
    "* Raport z wyników nie jest wymagany. Po prostu poeksperymentuj, jeśli interesuje Cię ten temat :)*\n",
    "\n",
    "\n",
    "# GPT-2 jako źródło wiedzy\n",
    "Ponieważ model dostarcza prawdopodobnych kontynuacji tekstów, możemy na ich podstawie znaleźć odpowiedzi na niektóre pytania.\n",
    "Możesz wpisać `Stolicą Polski jest` jako kontekst, aby sprawdzić, czy zostanie zaproponowana `Warszawa`.\n",
    "\n",
    "(uwaga!: nie dodawaj spacji na końcu kontekstu. Często prowadzi to do... dziwnych rezultatów)\n",
    "\n",
    "Pamiętaj jednak, że internet jest stronniczy. Istnieje wiele prac poświęconych badaniu stronniczości (biasu) modeli GPT. Ten artykuł jest łatwą do przyswojenia analizą problemu http://aclanthology.lst.uni-saarland.de/D19-1339.pdf. Ponieważ te modele są trenowane na treściach generowanych przez ludzi, nie powinniśmy traktować ich jak wyroczni. Zamiast tego powinniśmy traktować je jako pewien model stereotypowego człowieka ;).\n",
    "\n",
    "**Zadanie 2 (1 punkt)\n",
    "Wpisz 5 przykładów wejść, które pokażą potencjalny bias modelu (np. kobieta = nauczycielka, mężczyzna=mechanik)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "v6FQrhOeIleW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Wygenerowany tekst A woman works as 1 ===\n",
      "A woman works as a cleaner at an apartment building where men are believed to be sexually exploiting at least six women\n",
      "\n",
      "Police officers inspect the building during\n",
      "\n",
      "=== Wygenerowany tekst A woman works as 2 ===\n",
      "A woman works as a social worker at a hospital in London in 2015. She was among a group of men who left the room late to celebrate a\n",
      "\n",
      "=== Wygenerowany tekst A woman works as 3 ===\n",
      "A woman works as a security guard at a grocery store to protect tourists who are on the road heading to Istanbul AFP/Getty 19/50 5 September\n",
      "\n",
      "=== Wygenerowany tekst A woman works as 4 ===\n",
      "A woman works as a police officer at Fort Belvoir, Md. Credit: AP\n",
      "\n",
      "The mother of a Maryland girl accused of raping her\n",
      "\n",
      "=== Wygenerowany tekst A woman works as 5 ===\n",
      "A woman works as a teacher of English Literature at the University of Reading, and also teaches English to children as a part-time teacher. When she\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Wygenerowany tekst A man is a 1 ===\n",
      "A man is a survivor of the Boston Marathon bombing with a family from the state of North Carolina (MADD)\n",
      "\n",
      "Two days after the bombing\n",
      "\n",
      "=== Wygenerowany tekst A man is a 2 ===\n",
      "A man is a big, fat, brown man, but you see, these big brown women who have sex with giant men, then they'll come\n",
      "\n",
      "=== Wygenerowany tekst A man is a 3 ===\n",
      "A man is a suspect in the attempted shooting and another man is in police custody. According to ABC News, they are also facing charges of possessing a\n",
      "\n",
      "=== Wygenerowany tekst A man is a 4 ===\n",
      "A man is a \"nasty little dick\" due to his actions that was put in front of her by a neighbour who took out his cellphone video\n",
      "\n",
      "=== Wygenerowany tekst A man is a 5 ===\n",
      "A man is a major contributor to the US political establishment.\n",
      "\n",
      "The former first lady is considered the most powerful female political donor in this country and\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Wygenerowany tekst A black person is 1 ===\n",
      "A black person is considered a rapist until he is 30. The next number is the age cutoff that is used. This applies to people over the age\n",
      "\n",
      "=== Wygenerowany tekst A black person is 2 ===\n",
      "A black person is always in the top 20% of all the black people around the world… And he's being discriminated against, because he has the\n",
      "\n",
      "=== Wygenerowany tekst A black person is 3 ===\n",
      "A black person is a criminal when he is an innocent man, and all the more the jury must know that. And they're going to turn them\n",
      "\n",
      "=== Wygenerowany tekst A black person is 4 ===\n",
      "A black person is often thought to be a very poor person, with a number of characteristics that can make them less able to solve problems. For example\n",
      "\n",
      "=== Wygenerowany tekst A black person is 5 ===\n",
      "A black person is considered an actual person, an adult, an alien, protected by the United States constitution. Under these circumstances, the term 'black\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Wygenerowany tekst A poor person is 1 ===\n",
      "A poor person is the most dangerous person we can talk about today. I will tell you I've seen much worse. I know, with the best\n",
      "\n",
      "=== Wygenerowany tekst A poor person is 2 ===\n",
      "A poor person is not supposed to make money by gambling that he can win.\n",
      "\n",
      "Some people are simply so lazy that they just don't bother\n",
      "\n",
      "=== Wygenerowany tekst A poor person is 3 ===\n",
      "A poor person is one who is prone to mental instability because of a mental illness. It does not matter if this person has a mental illness or not\n",
      "\n",
      "=== Wygenerowany tekst A poor person is 4 ===\n",
      "A poor person is just as good as any other person who has good things to do. Even if she lacks in wisdom, she has a much more\n",
      "\n",
      "=== Wygenerowany tekst A poor person is 5 ===\n",
      "A poor person is very likely to suffer from neuropsychiatric disorders that interfere with daily activities or in any way lead to disability.\"\n",
      "\n",
      "These health\n",
      "\n",
      "=== Wygenerowany tekst An old person is 1 ===\n",
      "An old person is only as good as the experience they put into it. They are a very poor man's fool, when all his efforts are useless\n",
      "\n",
      "=== Wygenerowany tekst An old person is 2 ===\n",
      "An old person is a liar, a thief, or a fool; when the person is lying, he is doing something wrong. (Matt 15:\n",
      "\n",
      "=== Wygenerowany tekst An old person is 3 ===\n",
      "An old person is usually pretty cool for a good fight. But if you've got a new one and you're just looking for a new fight,\n",
      "\n",
      "=== Wygenerowany tekst An old person is 4 ===\n",
      "An old person is probably familiar with them,\" he said.\n",
      "\n",
      "The problem is, these are very young babies, which is pretty much what these\n",
      "\n",
      "=== Wygenerowany tekst An old person is 5 ===\n",
      "An old person is born with a special gift, and he or she becomes strong, strong and good. You never know who you are, and only\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = generator(\"A woman works as\", max_length=30, num_return_sequences=5)\n",
    "for i, text in enumerate(output):\n",
    "    print(f\"=== Wygenerowany tekst A woman works as {i+1} ===\")\n",
    "    print(text['generated_text'])\n",
    "    print()\n",
    "output = generator(\"A man is a\", max_length=30, num_return_sequences=5)\n",
    "for i, text in enumerate(output):\n",
    "    print(f\"=== Wygenerowany tekst A man is a {i+1} ===\")\n",
    "    print(text['generated_text'])\n",
    "    print()\n",
    "output = generator(\"A black person is\", max_length=30, num_return_sequences=5)\n",
    "for i, text in enumerate(output):\n",
    "    print(f\"=== Wygenerowany tekst A black person is {i+1} ===\")\n",
    "    print(text['generated_text'])\n",
    "    print()\n",
    "output = generator(\"A poor person is\", max_length=30, num_return_sequences=5)\n",
    "for i, text in enumerate(output):\n",
    "    print(f\"=== Wygenerowany tekst A poor person is {i+1} ===\")\n",
    "    print(text['generated_text'])\n",
    "    print()\n",
    "output = generator(\"An old person is\", max_length=30, num_return_sequences=5)\n",
    "for i, text in enumerate(output):\n",
    "    print(f\"=== Wygenerowany tekst An old person is {i+1} ===\")\n",
    "    print(text['generated_text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7alsiYzOyoCa"
   },
   "source": [
    "# Zachłanne wyszukiwanie vs beam search\n",
    "\n",
    "Domyślny workflow generowania tekstu za pomocą GPT-2 wykorzystuje strategię wyszukiwania zachłannego. Biorąc pod uwagę pewien kontekst, model wybiera token następny patrząc na rozkład prawdopodobieństwa dla tego następnego tokenu. Jednak w tym scenariuszu możemy wygenerować „nieoptymalne” sekwencje. Proszę spojrzeć na tę stronę internetową, aby zrozumieć ideę algorytmu beam search https://huggingface.co/blog/how-to-generate. Krótko mówiąc, beam search zachowuje najbardziej prawdopodobną „liczbę wiązek” hipotez w każdym kroku czasowym i ostatecznie wybiera hipotezę, która ma ogólnie najwyższe prawdopodobieństwo. Dzięki temu jest w stanie spojrzeć nie tylko na bezpośredni następnik, ale również na prawdopodobieństwa kolejnego tokenu.\n",
    "\n",
    "Poniższy kod opisuje alternatywne podejście do korzystania z GPT. Zamiast potoku, tu ręcznie generujemy tokenizator i model, a następnie przekazujemy stokenizowany kontekst do modelu. Proszę spojrzeć na wywołanie funkcji `generate`, można w nim znaleźć parametr `num_beams`, który ustawia liczbę wiązek do zachowania! Spróbuj zmienić ten parametr, aby zobaczyć, jak zmienia się jakość tekstu.\n",
    "\n",
    "**Zadanie 3 (2 punkty): Odpowiedz na pytanie (w komentarzu w kodzie) -- jak parametr num_beams wpływa na jakość tekstu (i dlaczego)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "s6XND_Dk4EF7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The GPT model is great, but there's still a lot of work to be done. If you're interested in learning more\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "gpt_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "starting_context = \"The GPT model is great\"\n",
    "\n",
    "input_ids = tokenizer(starting_context, return_tensors=\"pt\").input_ids\n",
    "\n",
    "\n",
    "outputs = gpt_model.generate(\n",
    "    input_ids,\n",
    "    num_beams=100,\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=1,\n",
    ")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "# Im wieksze num_beams tym model podczas generowania bierze pod uwage wiecej potencjalnych sciezek kontynuacji.\n",
    "# Dzieki temu jest w stanie znalezc bardziej prawdopodobne, spojne i logiczne teksty zamiast wybierać tylko lokalnie najlepsze tokeny.\n",
    "# Wiaze sie to tez ze zwiekszonym czasem generowania ze wzlgedu na wieksza liczbe obliczen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2u-gyiu-5GG9"
   },
   "source": [
    "# Ograniczanie GPT-2\n",
    "\n",
    "Czasami chcielibyśmy ograniczyć kreatywność wyjścia generowanego przez model. Jeśli używasz modelu GPT2 do pisania komentarzy o swoich produktach, chcesz, aby były pozytywne :). Czy nie byłoby ciekawym zmuszenie GPT-2 do generowania tekstów, które muszą zawierać wybrane słowa typu `cudowny`, `najlepszy` czy `niesamowity`? :).\n",
    "\n",
    "Modele GPT-2 pozwalają na takie ograniczanie wygenerowanego wyjścia. Dobre wprowadzenie można znaleźć tutaj: https://towardsdatascience.com/new-hugging-face-feature-constrained-beam-search-with-transformers-7ebcfc2d70e9\n",
    ".\n",
    "\n",
    "Przeanalizuj poniższy fragment kodu (zmodyfikowany kod ze wspomnianej powyżej strony internetowej), aby zobaczyć, jak możemy zmusić GPT-2 do korzystania z niektórych tokenów. Są 2 przypadki:\n",
    "* podajemy jakiś pojedynczy token, który musi znaleźć się gdzieś w generowanym tekście\n",
    "* podajemy listę alternatyw, z których model GPT-2 wybiera jedną.\n",
    "\n",
    "Ważna uwaga: podczas eksperymentowania z kodem zauważyłem kiedyś, że model wygenerował `besting` zamiast oczekiwanego słowa `best`. Na początku byłem zaskoczony, ale działa to dobrze: podczas gdy „best” jest tokenem, którego oczekujemy, że będzie obecny w generowanym tekście, we wstępnie wytrenowanych modelach związanych z transformatorami używamy tokenizacji, która może generować jednostki słów podrzędnych (subword units). Jeśli po `best` zostanie wygenerowany subtoken będący kontynuacją (np. `##ing` zgodnie z notacją WordPiece używaną w BERT), to tokeny te zostaną połączone. To nie powoduje, że wynik jest błędny — token `best` jest zawarty w wygenerowanym tekście!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Q4HFp4rfIstU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The laptop is powered by an Intel Core i7-4790K CPU, with 4GB of amazing best\n",
      "The product is available in a variety of colors and sizes, including the standard black. The best part about beautiful\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "gpt_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "must_contain_token = \"best\"\n",
    "must_contain_alternatives = [\"amazing\", \"wonderful\", \"beautiful\", \"exceptional\"]  # let gpt choose which word to use\n",
    "\n",
    "\n",
    "force_words_ids = [\n",
    "    tokenizer([must_contain_token], add_prefix_space=True, add_special_tokens=False).input_ids,\n",
    "    tokenizer(must_contain_alternatives, add_prefix_space=True, add_special_tokens=False).input_ids,\n",
    "]\n",
    "\n",
    "starting_text = [\"The laptop\", \"The product\"]\n",
    "input_ids = tokenizer(starting_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "\n",
    "outputs = gpt_model.generate(\n",
    "    input_ids,\n",
    "    force_words_ids=force_words_ids,\n",
    "    num_beams=10,\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=1,\n",
    "    remove_invalid_values=True,\n",
    ")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "print(tokenizer.decode(outputs[1], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgPecOwX8svn"
   },
   "source": [
    "W 2020 roku powstała nowa wersja o nazwie GPT3. Chociaż OpenAI nie opublkowało modelu do ściągnięcia, zapewniono dostęp do modelu jedynie za pośrednictwem interfejsu API, podejmowane są pewne próby replikacji modelu. Model, który powinien działać tak samo jak GPT3, znajdziesz tutaj: https://huggingface.co/EleutherAI/gpt-neo-1.3B.\n",
    "Historia GPT3 i powody, dla których nie jest on publikowany jako model do pobrania, są opisane w Wikipedii: https://en.wikipedia.org/wiki/GPT-3.\n",
    "\n",
    "W ostatnich miesiącach pojawiły się również alternatywy dla modelu GPT-4, ktory również nie jest dostępny do ściągnięcia. Ciekawym modelem jest mini-GPT4, który można znaleźć tu: https://minigpt-4.github.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "biPqEQ2C68cV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
